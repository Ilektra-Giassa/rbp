{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iterative Training + Mini Batch Hand Pick.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TmgfbhloQWh",
        "colab_type": "text"
      },
      "source": [
        "# Iterative Training with Hand Picked Mini-Batches\n",
        "This notebook contains functions to train a \n",
        "dense neural networks with iterative training.\n",
        "\n",
        "Iterative training is a strategy that loop over\n",
        "several users defined training phases. At each loop,\n",
        "the abundance of the negative class samples is increased by\n",
        "a user-defined ratio. This system allows us to gradually\n",
        "imbalanced a training set, while still keeping the learner\n",
        "able to recognize the positive class samples.\n",
        "\n",
        "Increasing the negative class samples reduced the \n",
        "probability of generating imbalanced mini-batches by random\n",
        "choice; meaning that in an imbalanced dataset of 100 positives and\n",
        "10000 negatives, there is the chance that Keras will create mini-batches\n",
        "of only negatives. Thus moving the weights of the neurons in favors of only the negative class samples. \n",
        "The notebook contains a helper function that allows to hand-pick mini-batches to keep at least one positive sample in each mini-batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5bIRL5f0KCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "bf62d2a7-888b-4934-bfba-a8f48c0f9fa7"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras as keras \n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNjlDTopbEw6",
        "colab_type": "text"
      },
      "source": [
        "## create random dataset\n",
        "This function is used only for the purpose\n",
        "of demostration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86kxeZpYbIwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_seq(length_a, length_b, label=None, alphabet=None):\n",
        "  list_a = [random.choice(alphabet) for i in range(0, length_a)]\n",
        "  list_b = [random.choice(alphabet) for i in range(0, length_b)]\n",
        "  seq_a, seq_b = ''.join(list_a), ''.join(list_b)\n",
        "  return [seq_a, seq_b, label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OzG9IhSUABr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Create simple CNN + FC\n",
        "Here we defined a simple convnet + fully connected layers\n",
        "that can be used for the demostration of the iterative\n",
        "training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4c9-x1FUAJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_arch_00b(input_tensor):\n",
        "  main_input = input_tensor\n",
        "  x = Conv2D(\n",
        "      filters=32,\n",
        "      kernel_size=(3, 3),\n",
        "      padding=\"same\",\n",
        "      data_format=\"channels_last\",\n",
        "      activation='relu',\n",
        "      name=\"conv_1\")(main_input)\n",
        "\n",
        "  x = MaxPooling2D(pool_size=(2, 2), name='max_1')(x)\n",
        "\n",
        "  x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "  conv_flat = Flatten(name='2d_matrix')(x)\n",
        "\n",
        "  x = Dense(128, activation='relu')(conv_flat)\n",
        "\n",
        "  x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "\n",
        "  x = Dropout(rate = 0.4)(x)\n",
        "\n",
        "  x = Dense(32, activation='relu')(x)\n",
        "\n",
        "  main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
        "\n",
        "  model = Model(inputs=[main_input], outputs=[main_output], name='arch_00b')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mkKfxm9ZNP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seeVC6hp9ZWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_log(record_training, log_path):\n",
        "  col_names = \"MAX_ITER,model_name,OPTIMIZER,model.name,PICK_MINIBATCHES,iteration,POS_SAMPLES,neg_ratio,neg_samples\".split(',')\n",
        "  df_log = pd.DataFrame(\n",
        "      record_training,\n",
        "      columns=col_names)\n",
        "  df_log.to_csv(log_path, header=True, index=False, sep='\\t')\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdx6mBovO17V",
        "colab_type": "text"
      },
      "source": [
        "## create dot matrix from pandas df\n",
        "\n",
        "This function converts a pandas dataframe with columns named ['seq_a', 'seq_b']\n",
        "into an array of dot matrix. Each watson-crick pair got a score of 1, else 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzL15uMHO2GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(df, tensor_dim, log=False):\n",
        "  \"\"\"\n",
        "  fun transform input database to\n",
        "  one hot encoding array.\n",
        "  paramenters:\n",
        "  df=input dataset\n",
        "  tensor_dim=tensor dimension as tuple\n",
        "  log=log time [bool(false)]\n",
        "  \"\"\"\n",
        "  def logs(index, start): # logs\n",
        "    end = time.time()\n",
        "    print(\"processed rows are,\", index, sep='\\t')\n",
        "    print(\"elapsed time(sec) is:\", end - start, sep='\\t')\n",
        "  \n",
        "  # warning: any nucleotide sequence is converted to capital letters\n",
        "\n",
        "  ## alphabet for watson-crick interactions.\n",
        "  alphabet = {\"AT\": 1, \"TA\": 1, \"GC\": 1, \"CG\": 1}\n",
        "  X = df.reset_index(drop=True)\n",
        "  # convert sample's labels to array\n",
        "  y_ohe = X.label.to_numpy()\n",
        "  # create empty dot matrix\n",
        "  dot_matrix_ohe = np.zeros(\n",
        "      (df.shape[0], *tensor_dim), dtype=\"float32\"\n",
        "      )\n",
        "  # some time logs\n",
        "  start = time.time()\n",
        "  \n",
        "  # loop over samples; improvement todo\n",
        "  for index, sample in X.iterrows():\n",
        "    # loop over nucleotides of sequence_a\n",
        "    for seq_a_idex, seq_a_nt in enumerate(sample.seq_a.upper(), start=0):\n",
        "      # loop over nucleotides of sequence_b\n",
        "      for seq_b_index, seq_b_nt in enumerate(sample.seq_b.upper(), start=0):\n",
        "        pair = seq_a_nt + seq_b_nt\n",
        "        dot_matrix_ohe[index, seq_a_idex, seq_b_index, 0] = alphabet.get(pair, 0)\n",
        "        # print some logging\n",
        "        if log:\n",
        "          if index % 1000 == 0:\n",
        "            logs(index, start)\n",
        "  return [dot_matrix_ohe, y_ohe]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQR_5_bYOfZ-",
        "colab_type": "text"
      },
      "source": [
        "## Dataset generator for iterative training\n",
        "`generate_subset` is a function that subset N samples\n",
        "from the original dataset, and returns a new subset + the\n",
        "original dataset (with subset samples removed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw3LeCp8M1cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### create training set:\n",
        "def generate_subset(df, N, drop=True):\n",
        "  # copy original input df, this avoid\n",
        "  # to corrupt original df.\n",
        "  X = df.copy()\n",
        "  # pick random N samples from main X\n",
        "  subset = X.sample(\n",
        "          n=N\n",
        "      )\n",
        "  \n",
        "  if drop: # remove sample\n",
        "    # get indexes for picked samples\n",
        "    subset_samples_index = subset.index.tolist()\n",
        "    # remove picked samples from original X\n",
        "    new_X = X.drop(subset_samples_index, axis=0)\n",
        "  else: # do nothing\n",
        "    new_X = X\n",
        "  # reset indexes\n",
        "  subset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return subset, new_X\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYQaGWlaYC-p",
        "colab_type": "text"
      },
      "source": [
        "## train script\n",
        "This function takes as input a list [batches_list] containing hand-picked\n",
        "minibatches, and train the model [model] by fittin each mini-batch. Additionaly,\n",
        "you can provide an array with sample weights as well as class weigths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTFckOTLM1x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(\n",
        "    model, \n",
        "    pick_minibatch=True,\n",
        "    batches_list=None,\n",
        "    sample_weight=None,\n",
        "    class_weight=None,\n",
        "    reset_metrics=True,\n",
        "  ):\n",
        "  if pick_minibatch:\n",
        "    # train model by looping over minibatches\n",
        "    for batch in batches_list:\n",
        "      ## assign samples and labels\n",
        "      X_ohe, y_ohe = batch\n",
        "      ## train model\n",
        "      ## read more about parameters at:\n",
        "      ## https://keras.io/models/model/\n",
        "      model.train_on_batch(\n",
        "          { \"main_input\" : X_ohe},\n",
        "          { \"main_output\" : y_ohe},\n",
        "          sample_weight=sample_weight,\n",
        "          class_weight=class_weight,\n",
        "          reset_metrics=reset_metrics\n",
        "        )\n",
        "    return model\n",
        "  else:\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isUNT_lLuNpA",
        "colab_type": "text"
      },
      "source": [
        "# create a fake dataset\n",
        "this cell uses the random_seq() function defined\n",
        "above to create a fake dataset for the positive\n",
        "sample class and one fake dataset for the negative\n",
        "sample class. Has you can see, it is compulsary to \n",
        "keep the two dataset separated. When we hand-pick \n",
        "mini-batches, we will first split the two datasets\n",
        "into subsets. Then, each positive class sample\n",
        "subset will be unified with one negative class sample\n",
        "to create a minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5FJipT4uNxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create fake positive class dataset\n",
        "seq_pos_list = [ random_seq(50, 20, label=1, alphabet=list('ACGT')) for i in range(0, 5000)]\n",
        "seq_neg_list = [ random_seq(50, 20, label=0, alphabet=list('N')) for i in range(0, 10000)]\n",
        "\n",
        "\n",
        "df_pos_orig = pd.DataFrame(seq_pos_list, columns = ['seq_a', 'seq_b', 'label'])\n",
        "df_neg_orig = pd.DataFrame(seq_neg_list, columns = ['seq_a', 'seq_b', 'label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpl7HsiDYWJe",
        "colab_type": "text"
      },
      "source": [
        "# iterative train\n",
        "**Iterative** **training** is a sequence of steps to re-training the same model,while increase the abundance of the negative class. For this purpose, and to avoid overfitting, both samples from positive and negative class are replaced after each iteration. Thus, the training dataset is updated at each iteration.\n",
        "\n",
        "The function that cares to update the training dataset is `generate_subset`. This function takes as input the original dataset `df` and an integer `N`. `N` corresponds to the number of samples that we want to subsamples. The function subsamples `N` samples from the original dataset `df`, thus creating a new subset `df_sub`. The function returns the newly create subset `df_sub` + the original dataset. The original dataset **will** **not** contain the subsampled samples, thus will have a size equal to: original dataset size - N. Which means\n",
        "that after several iteration the original dataset will decrease to size zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnZdPx4eYelj",
        "colab_type": "text"
      },
      "source": [
        "## config static variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jkKVeoFYhex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "6086b45d-4b40-4fa0-fb17-bda63ff3d6cf"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "## tensor dim\n",
        "tensor_dim = (50, 20, 1) # we define a tensor of dimention 50, 20, 1 that\n",
        "                         # correspond to the dimension of the dot matrix \n",
        "                         # (seq_a has length 50, seq_b length 20, and 1 dimention\n",
        "                         # to store the watson-crick value)\n",
        "\n",
        "## input keras tensor\n",
        "input_tensor = Input(\n",
        "      shape=tensor_dim, # the tensor dimentions (see above)\n",
        "      dtype='float32', # array of float32 is ok for our purposes and allows to save memory.\n",
        "      name='main_input' # define a name for the tensor.\n",
        "    )\n",
        "\n",
        "# create the model\n",
        "architecture = make_arch_00b(input_tensor)\n",
        "OPTIMIZER = 'RMSprop'\n",
        "\n",
        "# settings train strategy with\n",
        "# hand picked minibatch\n",
        "PICK_MINIBATCHES = True # bool\n",
        "SPLIT_BATCH = 10 # set number of minibatches, which means that the two main \n",
        "                 # datasets storing the positive and negative class,\n",
        "                 # respectively, will be diveded into N==SPLIT_BATCH\n",
        "                 # subsets.\n",
        "\n",
        "# DATASET STATIC VARIABLES\n",
        "POS_SAMPLES = 100\n",
        "NEG_RATIO_MINISTEPS = 0.5 # define mini-steps to increase\n",
        "                          # negative class sample ratio\n",
        "MAX_ITER = 25\n",
        "NEG_RATIO_START = 1 # initial negative class sample ratio to start with.\n",
        "\n",
        "# set folders and logs\n",
        "RECORD_TRAINING = list()\n",
        "save_ = True # if true save models and training parameters\n",
        "if save_ :\n",
        "  RECORD_FILENAME = 'iterative_training_paramenters.csv'\n",
        "  LOG_DIR = './'\n",
        "  MODEL_PATH = './'\n",
        "  LOG_PATH = LOG_DIR + RECORD_FILENAME"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDayx3-Y4aq",
        "colab_type": "text"
      },
      "source": [
        "### run iterative training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2WOCtHkKhFi",
        "colab_type": "code",
        "outputId": "070123d9-bb68-4158-ed38-4aa8c51bd8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# iterative training\n",
        "for iteration in range(0, MAX_ITER):\n",
        "  try:\n",
        "    if iteration == 0:\n",
        "      neg_ratio = NEG_RATIO_START\n",
        "      print('# model initialization..')\n",
        "      model = architecture # initialize model\n",
        "      model.compile(optimizer=OPTIMIZER,\n",
        "                loss='binary_crossentropy',\n",
        "                )\n",
        "    else:\n",
        "      # increase negative class sample ratio for this iteration      \n",
        "      neg_ratio += NEG_RATIO_MINISTEPS  \n",
        "    # assign model name for iteration n\n",
        "    model_name = f'model_{model.name}_{iteration}'\n",
        "    # define number of negative class samples \n",
        "    # that will be picked up.\n",
        "    neg_samples = int(neg_ratio * POS_SAMPLES)\n",
        "\n",
        "    print(\"# model name is:\", model_name, sep='\\t')\n",
        "    print(\"# iteration is:\", iteration, sep='\\t')\n",
        "    print(\"## pos.class main dataset size is:\", df_pos_orig.shape, sep='\\t')\n",
        "    print(\"## neg.class main dataset size is:\", df_neg_orig.shape, sep='\\t')\n",
        "\n",
        "    print(\"## neg.class ratio is:\", neg_ratio, sep='\\t')\n",
        "    print(\"## pos.class samples size is:\", POS_SAMPLES, sep='\\t')\n",
        "    print(\"## neg.class samples size is:\", neg_samples, sep='\\t')\n",
        "\n",
        "    # GENERATE TRAINING DATASET\n",
        "    ## subsample pos.class samples\n",
        "    ## from original pos.class dataset.\n",
        "    subset_pos, df_pos_orig = generate_subset(\n",
        "        df_pos_orig, N=POS_SAMPLES\n",
        "        )\n",
        "    ## subsample neg.class samples\n",
        "    ## from original neg.class dataset\n",
        "    subset_neg, df_neg_orig = generate_subset(\n",
        "          df_neg_orig,\n",
        "          N=neg_samples,\n",
        "      )\n",
        "\n",
        "    ## generate hand-picked imbalanced mini-batches\n",
        "    if PICK_MINIBATCHES:\n",
        "      print('### pick minibatches')\n",
        "      batches_list = []\n",
        "      ## split subset_pos.class into minibatches\n",
        "      ### see numpy doc for more details:\n",
        "      # https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html\n",
        "      batch_pos = np.array_split(subset_pos, SPLIT_BATCH)\n",
        "      ## split subset_neg into minibatches\n",
        "      ## of size == SPLIT_BATCH\n",
        "      batch_neg = np.array_split(subset_neg, SPLIT_BATCH)\n",
        "      ## zip together pos and neg subsets to create minibatches\n",
        "      for mini_index, minibatch_pairs in enumerate(zip(batch_pos, batch_neg)):\n",
        "        print('### minibatch pair id is:', mini_index,\n",
        "              'pos shape is:', minibatch_pairs[0].shape[0],\n",
        "              'neg_shape_is:', minibatch_pairs[1].shape[0],\n",
        "              sep='\\t'\n",
        "              )\n",
        "\n",
        "        batch_train = pd.concat(minibatch_pairs)\n",
        "        # converts to dot matrix | input for model\n",
        "        minibatch = one_hot_encoding(batch_train, tensor_dim)\n",
        "        # append each minibatch to minibatch list\n",
        "        batches_list.append(minibatch)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "    # train model\n",
        "    model = train(\n",
        "      model,\n",
        "      pick_minibatch=PICK_MINIBATCHES,\n",
        "      batches_list=batches_list,\n",
        "      )\n",
        "      \n",
        "    # # create log_parameters\n",
        "    log_parameters = [\n",
        "        MAX_ITER,\n",
        "        model_name, \n",
        "        OPTIMIZER,\n",
        "        model.name,\n",
        "        PICK_MINIBATCHES,\n",
        "        iteration,\n",
        "        POS_SAMPLES,\n",
        "        neg_ratio,\n",
        "        neg_samples\n",
        "        \n",
        "    ]\n",
        "    RECORD_TRAINING.append(log_parameters)\n",
        "    if save_:\n",
        "      print('## save model as:', f'{MODEL_PATH}/{model_name}.h5')\n",
        "      # save model at current iteration stage\n",
        "      model.save(f'{MODEL_PATH}/{model_name}.h5')\n",
        "      print('## save log as:', LOG_PATH)\n",
        "      # save log\n",
        "      save_log(RECORD_TRAINING, LOG_PATH)\n",
        "  except ValueError as err:\n",
        "    print('raised ValueError:', err)\n",
        "    print('it may be possible that you run out of samples from main datasets')\n",
        "    break\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# model initialization..\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "# model name is:\tmodel_arch_00b_0\n",
            "# iteration is:\t0\n",
            "## pos.class main dataset size is:\t(5000, 3)\n",
            "## neg.class main dataset size is:\t(10000, 3)\n",
            "## neg.class ratio is:\t1\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t100\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t10\n",
            "## save model as: .//model_arch_00b_0.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_1\n",
            "# iteration is:\t1\n",
            "## pos.class main dataset size is:\t(4900, 3)\n",
            "## neg.class main dataset size is:\t(9900, 3)\n",
            "## neg.class ratio is:\t1.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t150\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t15\n",
            "## save model as: .//model_arch_00b_1.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_2\n",
            "# iteration is:\t2\n",
            "## pos.class main dataset size is:\t(4800, 3)\n",
            "## neg.class main dataset size is:\t(9750, 3)\n",
            "## neg.class ratio is:\t2.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t200\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t20\n",
            "## save model as: .//model_arch_00b_2.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_3\n",
            "# iteration is:\t3\n",
            "## pos.class main dataset size is:\t(4700, 3)\n",
            "## neg.class main dataset size is:\t(9550, 3)\n",
            "## neg.class ratio is:\t2.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t250\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t25\n",
            "## save model as: .//model_arch_00b_3.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_4\n",
            "# iteration is:\t4\n",
            "## pos.class main dataset size is:\t(4600, 3)\n",
            "## neg.class main dataset size is:\t(9300, 3)\n",
            "## neg.class ratio is:\t3.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t300\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t30\n",
            "## save model as: .//model_arch_00b_4.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_5\n",
            "# iteration is:\t5\n",
            "## pos.class main dataset size is:\t(4500, 3)\n",
            "## neg.class main dataset size is:\t(9000, 3)\n",
            "## neg.class ratio is:\t3.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t350\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t35\n",
            "## save model as: .//model_arch_00b_5.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_6\n",
            "# iteration is:\t6\n",
            "## pos.class main dataset size is:\t(4400, 3)\n",
            "## neg.class main dataset size is:\t(8650, 3)\n",
            "## neg.class ratio is:\t4.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t400\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t40\n",
            "## save model as: .//model_arch_00b_6.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_7\n",
            "# iteration is:\t7\n",
            "## pos.class main dataset size is:\t(4300, 3)\n",
            "## neg.class main dataset size is:\t(8250, 3)\n",
            "## neg.class ratio is:\t4.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t450\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t45\n",
            "## save model as: .//model_arch_00b_7.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_8\n",
            "# iteration is:\t8\n",
            "## pos.class main dataset size is:\t(4200, 3)\n",
            "## neg.class main dataset size is:\t(7800, 3)\n",
            "## neg.class ratio is:\t5.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t500\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t50\n",
            "## save model as: .//model_arch_00b_8.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_9\n",
            "# iteration is:\t9\n",
            "## pos.class main dataset size is:\t(4100, 3)\n",
            "## neg.class main dataset size is:\t(7300, 3)\n",
            "## neg.class ratio is:\t5.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t550\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t55\n",
            "## save model as: .//model_arch_00b_9.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_10\n",
            "# iteration is:\t10\n",
            "## pos.class main dataset size is:\t(4000, 3)\n",
            "## neg.class main dataset size is:\t(6750, 3)\n",
            "## neg.class ratio is:\t6.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t600\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t60\n",
            "## save model as: .//model_arch_00b_10.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_11\n",
            "# iteration is:\t11\n",
            "## pos.class main dataset size is:\t(3900, 3)\n",
            "## neg.class main dataset size is:\t(6150, 3)\n",
            "## neg.class ratio is:\t6.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t650\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t65\n",
            "## save model as: .//model_arch_00b_11.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_12\n",
            "# iteration is:\t12\n",
            "## pos.class main dataset size is:\t(3800, 3)\n",
            "## neg.class main dataset size is:\t(5500, 3)\n",
            "## neg.class ratio is:\t7.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t700\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t70\n",
            "## save model as: .//model_arch_00b_12.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_13\n",
            "# iteration is:\t13\n",
            "## pos.class main dataset size is:\t(3700, 3)\n",
            "## neg.class main dataset size is:\t(4800, 3)\n",
            "## neg.class ratio is:\t7.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t750\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t75\n",
            "## save model as: .//model_arch_00b_13.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_14\n",
            "# iteration is:\t14\n",
            "## pos.class main dataset size is:\t(3600, 3)\n",
            "## neg.class main dataset size is:\t(4050, 3)\n",
            "## neg.class ratio is:\t8.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t800\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t80\n",
            "## save model as: .//model_arch_00b_14.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_15\n",
            "# iteration is:\t15\n",
            "## pos.class main dataset size is:\t(3500, 3)\n",
            "## neg.class main dataset size is:\t(3250, 3)\n",
            "## neg.class ratio is:\t8.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t850\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t85\n",
            "## save model as: .//model_arch_00b_15.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_16\n",
            "# iteration is:\t16\n",
            "## pos.class main dataset size is:\t(3400, 3)\n",
            "## neg.class main dataset size is:\t(2400, 3)\n",
            "## neg.class ratio is:\t9.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t900\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t90\n",
            "## save model as: .//model_arch_00b_16.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_17\n",
            "# iteration is:\t17\n",
            "## pos.class main dataset size is:\t(3300, 3)\n",
            "## neg.class main dataset size is:\t(1500, 3)\n",
            "## neg.class ratio is:\t9.5\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t950\n",
            "### pick minibatches\n",
            "### minibatch pair id is:\t0\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t1\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t2\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t3\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t4\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t5\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t6\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t7\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t8\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "### minibatch pair id is:\t9\tpos shape is:\t10\tneg_shape_is:\t95\n",
            "## save model as: .//model_arch_00b_17.h5\n",
            "## save log as: ./iterative_training_paramenters.csv\n",
            "# model name is:\tmodel_arch_00b_18\n",
            "# iteration is:\t18\n",
            "## pos.class main dataset size is:\t(3200, 3)\n",
            "## neg.class main dataset size is:\t(550, 3)\n",
            "## neg.class ratio is:\t10.0\n",
            "## pos.class samples size is:\t100\n",
            "## neg.class samples size is:\t1000\n",
            "raised ValueError: Cannot take a larger sample than population when 'replace=False'\n",
            "it may be possible that you run out of samples from main datasets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}